{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_token = '<SRC>'\n",
    "ref_token = '<REF>'\n",
    "cnd_token = '<CND>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources    = []\n",
    "references = []\n",
    "candidates = []\n",
    "scores     = []\n",
    "labels     = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', 'r', encoding='utf8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    num_lines = len(lines)\n",
    "    \n",
    "    lines = (l for l in lines)\n",
    "    \n",
    "    for _ in range(num_lines // 6):\n",
    "        sources.append(next(lines))\n",
    "        references.append(next(lines))\n",
    "        candidates.append(next(lines))\n",
    "        scores.append(float(next(lines)))\n",
    "        labels.append(next(lines))\n",
    "        next(lines) # ignore newline char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 272\n",
      "Baseline accuracy:  0.5342465753424658\n",
      "[1.8717948717948718, 2.1470588235294117]\n"
     ]
    }
   ],
   "source": [
    "# Check frequencies of each class\n",
    "num_H = sum([1 if x == 'H' else 0 for x in labels])\n",
    "num_M = sum([1 if x == 'M' else 0 for x in labels])\n",
    "\n",
    "frac_pos = num_M / (num_H + num_M)\n",
    "frac_neg = 1.0 - frac_pos\n",
    "loss_weights = [1 / frac_neg, 1 / frac_pos]\n",
    "\n",
    "print(num_H, num_M)\n",
    "print('Baseline accuracy: ', num_H / (num_H + num_M) )\n",
    "print(loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join reference and candidate sentences\n",
    "english_tokens = [(' %s ' % ref_token).join(ref_cand) for ref_cand in zip(references, candidates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bahraini princess marries us soldier , astonishing 5 year bond comes to end <REF> bahraini princess marries a u.s. soldier ; astounding marriage dissolves in 5 years\n"
     ]
    }
   ],
   "source": [
    "print(english_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use text8's word embeddings\n",
    "import gensim\n",
    "save_path = os.path.join(os.getcwd(), \"../text8.model\")\n",
    "embeddings = gensim.models.Word2Vec.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentences, labels, bleu_scores):\n",
    "    '''\n",
    "    Converts each token in each document in sentences to a vector using text8 word embeddings.\n",
    "    \n",
    "    Each sentence becomes a 256 x max_sentence_len matrix\n",
    "    '''\n",
    "    \n",
    "    # Each token is turned into a 256 float vector\n",
    "    embedding_length = 256\n",
    "    max_sentence_len = max(len(s) for s in sentences)\n",
    "    \n",
    "    # For smaller sentences, each unused column in the matrix is padded\n",
    "    pad_val = 100\n",
    "    \n",
    "    vectorized = []\n",
    "    \n",
    "    # Loop through every token in every sentence and convert to word embedding\n",
    "    for sentence, label, score in zip(sentences, labels, bleu_scores):\n",
    "        \n",
    "        vector_array = torch.zeros(max_sentence_len, embedding_length)\n",
    "        label_val    = torch.LongTensor([0 if label == 'H' else 1])\n",
    "        bleu_score   = torch.FloatTensor([score])\n",
    "        \n",
    "        \n",
    "        for i,word in enumerate(sentence):\n",
    "            \n",
    "            # Special case\n",
    "            # if word == ref_token:\n",
    "            #    vector_array[i,:] = torch.ones(embedding_length)\n",
    "            \n",
    "            # Unknown case\n",
    "            if word not in embeddings: \n",
    "                continue\n",
    "            \n",
    "            # Known case\n",
    "            vector_array[i,:] = torch.from_numpy(embeddings[word])\n",
    "        \n",
    "        vectorized.append( (vector_array, label_val, bleu_score) )\n",
    "    \n",
    "    return vectorized\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tgpen\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Users\\tgpen\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:36: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "..\\torch\\csrc\\utils\\tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
     ]
    }
   ],
   "source": [
    "vector_tokens = vectorize(english_tokens, labels, scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.X = torch.cat([X.unsqueeze(0) for X, _, _ in data])\n",
    "        self.y = torch.cat([y for _, y, _ in data])\n",
    "        self.bleu = torch.cat([b for _, _, b in data])\n",
    "        self.len = len(data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index], self.bleu[index]\n",
    "\n",
    "def get_data_loaders(train, val, batch_size=16):\n",
    "    \n",
    "    dataset = CustomDataset(train + val)\n",
    "\n",
    "    train_indices = [i for i in range(len(train))]\n",
    "    val_indices   = [i for i in range(len(train), len(train) + len(val))]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    train_loader  = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    \n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    val_loader  = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden=64, bi=False, drop=0.0, layers=1):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim  = 256\n",
    "        self.output_dim = 2\n",
    "        self.hidden     = hidden\n",
    "        self.bi         = bi\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.input_dim,\n",
    "                           hidden_size=self.hidden,\n",
    "                           num_layers=layers,\n",
    "                           bidirectional=bi,\n",
    "                           dropout=drop)\n",
    "        \n",
    "        self.fc         = nn.Linear(1 + 2 * hidden, self.output_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax    = nn.LogSoftmax(dim=1)\n",
    "        self.loss       = nn.CrossEntropyLoss(torch.FloatTensor(loss_weights))\n",
    "        self.dropout    = nn.Dropout(drop)\n",
    "    \n",
    "    def forward(self, inputs, bleu):\n",
    "        \n",
    "        batch_size = inputs.size()[0]\n",
    "        embed_size = 256\n",
    "        pad_val    = 100\n",
    "        \n",
    "        # A function to determine which columns in the sentence matrix are dummy paddings\n",
    "        mask = lambda x: torch.nonzero(x != pad_val)\n",
    "        \n",
    "        # Get the true length of every sentence in the batch\n",
    "        lengths = torch.tensor( [mask(inputs[i][:,0]).size()[0] for i in range(batch_size)] )\n",
    "        \n",
    "        inputs     = inputs.transpose(1, 0)\n",
    "        input_size = inputs.size()\n",
    "        \n",
    "        # Remove all dummy padding sentences so only relevant data is fed into LSTM\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(inputs, lengths,\n",
    "                                                  batch_first=False, enforce_sorted=False)\n",
    "        \n",
    "        output, (hn, _) = self.lstm(packed)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        \n",
    "        if self.bi:\n",
    "            fc_input = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            fc_input = hn.squeeze(0)\n",
    "        \n",
    "        bleu = torch.reshape(bleu, (len(bleu), 1))\n",
    "        fc_input = torch.cat((fc_input, bleu), 1)\n",
    "        \n",
    "        fc_output = self.fc(self.dropout(fc_input))\n",
    "        predicted = self.softmax(fc_output)\n",
    "        \n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer):\n",
    "    \n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct    = 0\n",
    "    total      = 0\n",
    "    \n",
    "    for (input_batch, expected_out, bleu) in tqdm(train_loader, leave=False, desc=\"Training Batches\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_batch.to(device), bleu.to(device)).to(device)\n",
    "        total  += outputs.size()[0]\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Calc how many correct predictions\n",
    "        match = (expected_out.to('cpu') == predicted.to('cpu')).cpu().numpy().sum()\n",
    "        correct += match\n",
    "        \n",
    "        loss = model.loss(outputs.to(device), expected_out.to(device))\n",
    "        total_loss += loss\n",
    "        \n",
    "        # Gradient descent\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('Train Acc:', correct / total)\n",
    "    print('Train loss:', float(total_loss))\n",
    "    \n",
    "def evaluate(model, val_loader):\n",
    "    \n",
    "    # Set the model to eval mode to avoid gradient calcs\n",
    "    model.eval()\n",
    "    \n",
    "    true_pos  = 0\n",
    "    true_neg  = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for (input_batch, expected_out, bleu) in tqdm(val_loader, leave=False, desc=\"Validation\"):\n",
    "\n",
    "        outputs = model(input_batch.to(device), bleu.to(device)).to(device)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        loss = model.loss(outputs.to(device), expected_out.to(device))\n",
    "        \n",
    "        # Keep track of total confusion matrix statistics\n",
    "        gold = expected_out.to('cpu').numpy()\n",
    "        pred = predicted.to('cpu').numpy()\n",
    "        CM   = confusion_matrix(gold, pred)\n",
    "        true_neg  += CM[0][0]\n",
    "        false_neg += CM[1][0]\n",
    "        true_pos  += CM[1][1]\n",
    "        false_pos += CM[0][1]\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    recall    = true_pos / (true_pos + false_neg)\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    f1        = 2.0 * precision * recall / (precision + recall)\n",
    "    print('Validation F1 score:', f1)\n",
    "    return f1\n",
    "\n",
    "def train_and_evaluate(num_epochs, model, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    f1 = 0\n",
    "    for epoch in trange(num_epochs, desc=\"Epochs\"):\n",
    "        train_epoch(model, train_loader, optimizer)\n",
    "        f1 = evaluate(model, val_loader)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "total_f1 = 0\n",
    "for i in range(5):\n",
    "    random.shuffle(vector_tokens)\n",
    "    split = int(0.80 * len(vector_tokens))\n",
    "    train_loader, val_loader = get_data_loaders(vector_tokens[:split], \n",
    "                                                vector_tokens[split:],\n",
    "                                                batch_size=32)\n",
    "\n",
    "    model = LSTM(hidden=64, bi=True, layers=3, drop=0.05).to(device)\n",
    "\n",
    "    total_f1 += train_and_evaluate(20, model, train_loader)\n",
    "    \n",
    "print('Average F1 score:', total_f1 / 5.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
